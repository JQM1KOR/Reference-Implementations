{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31764cf9",
   "metadata": {},
   "source": [
    "# `Classification Model`\n",
    "* <b>File Name</b>                          : Reference_implementation of covid model in Azure ML Studio\n",
    "* <b>Date of creation(dd-mm-yyyy)</b>       : 14-09-2022\n",
    "* <b>Author Name/Dept</b>                   : AIShield\n",
    "* <b>Organization</b>                       : BGSW\n",
    "* <b>Description</b>                        : Source Code of reference implementation\n",
    "* <b>Copyright</b>                          : Copyright 2022 Bosch Global Software Technologies Private Limited. All Rights Reserved.\n",
    "\n",
    "###  `Metadata`\n",
    "* Dataset: covid dataset\n",
    "* Size of training set: 250\n",
    "* Size of testing set : 25\n",
    "* Number of class : 2\n",
    "* Original Model: CNN\n",
    "\n",
    "### `Outcomes`\n",
    "* Accuracy of model: 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d749c77",
   "metadata": {},
   "source": [
    "### `1.1 Install the pre-requisite packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57614eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.24.1\n",
    "# !pip install pandas==1.15\n",
    "# !pip install requests==2.28.2\n",
    "# !pip install google-cloud-aiplatform==1.21.0\n",
    "# !pip install google-cloud-storage==2.7.0\n",
    "# !pip install google-auth-oauthlib==0.4.6\n",
    "# !pip install Pillow==9.4.0\n",
    "# !pip install opencv-python==4.7.0.68\n",
    "# !pip install tensorflow==2.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f705bce",
   "metadata": {},
   "source": [
    "### `1.2 Install gcloud sdk (to run on jupyter-notebook `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff29507",
   "metadata": {},
   "source": [
    "Download the Google Cloud CLI Installer, and Install the same (can use gcloud commands which needed to upload download files on gcloud storage)\n",
    "1. https://cloud.google.com/sdk/docs/install\n",
    "2. open the terminal and do gcloud init --console-only (This will initialize the gcloud configuration and will autheticate the same)\n",
    "3. refer the below link to installation of gcloud sdk (https://www.youtube.com/watch?v=PY8KnoCJpjo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cfae8",
   "metadata": {},
   "source": [
    "## 1.0 Importing neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adede4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : Importing libraries\n",
    "'''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a8d68",
   "metadata": {},
   "source": [
    "## 2.0 Setting up gcloud instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7d788",
   "metadata": {},
   "source": [
    "### Authenticate your Google Cloud account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c29557",
   "metadata": {},
   "source": [
    "#### 1. Vertex AI Workbench:\n",
    "\n",
    "`Do nothing as you are already authenticated.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ee13f",
   "metadata": {},
   "source": [
    "#### 2. Local JupyterLab instance, uncomment and run:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572b6683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : to authenticate gcloud login\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : to authenticate gcloud login\n",
    "'''\n",
    "\n",
    "! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed546b",
   "metadata": {},
   "source": [
    "#### 3. Colab, uncomment and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77a6fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : to authenticate gcloud login in colab\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : to authenticate gcloud login in colab\n",
    "'''\n",
    "\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36996b15",
   "metadata": {},
   "source": [
    "###  `2.0.1 To access and run gcloud applications instances locally in jupyter-notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486bed6",
   "metadata": {},
   "source": [
    "#### 1. Vertex AI Workbench:\n",
    "\n",
    "`Do nothing as you are already authenticated.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e525f42",
   "metadata": {},
   "source": [
    "#### 2. Local JupyterLab instance, uncomment and run:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c90fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : to authenticate gcloud application login\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : to authenticate gcloud application login\n",
    "'''\n",
    "\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0deb2f",
   "metadata": {},
   "source": [
    "### Setting the application credentials as environmetal variable to use gcloud applications\n",
    "\n",
    "1. when we run the command !gcloud auth application-default login\n",
    "2. it will save the credentials to a file , can found as the output of running the above command.\n",
    "3. **copy that path we got , and set as an environmental varibales to access and use the applications of gcloud** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9e945d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : to set the credentials of gcloud application login to enviromnetal vaibale \n",
    "'''\n",
    "\n",
    "## TODO copy the path we got as output of the above cell \n",
    "google_application_credentials_path = r\"<PATH>\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = google_application_credentials_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab7d83",
   "metadata": {},
   "source": [
    "#### 3. Colab, uncomment and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2206e34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : to authenticate gcloud application login in colab\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : to authenticate gcloud application login in colab\n",
    "'''\n",
    "\n",
    "# !pip install -U -q PyDrive\n",
    "\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbef7e1",
   "metadata": {},
   "source": [
    "### `2.0.2 Set the Prpject ID , to jupyter-notebook`\n",
    "\n",
    "###### this is the below project_id and region we will use while creating any job instance or bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d86a7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Description : Set the Project id \n",
    "'''\n",
    "\n",
    "##TODO copy and paste project_id in plcae of <PROJECT_ID>\n",
    "PROJECT_ID = \"<PROJECT_ID>\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dcb505bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : Set the region and bucket name\n",
    "'''\n",
    "\n",
    "##TODO copy and paste region , bucket_name  in plcae of <REGION_NAME> and <BUCKET_NAME>\n",
    "\n",
    "REGION = \"<REGION_NAME>\" # @param {type:\"string\"}\n",
    "BUCKET_NAME = \"<BUCKET_NAME>\" # @param {type:\"string\"}\n",
    "BUCKET_URI = \"gs://\"+\"<BUCKET_NAME>\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f18ee",
   "metadata": {},
   "source": [
    "### uncomment  the below cell , if you want to create a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cbad978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO copy and paste the necessary info in {REGION}, {PROJECT_ID} AND {BUCKE_URI}\n",
    "\n",
    "# !gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701f6a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : this will print the list of buckets present respective to the given PROJECT_ID\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : this will print the list of buckets present respective to the given PROJECT_ID\n",
    "'''\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "buckets = storage_client.list_buckets()\n",
    "print(\"Buckets:\")\n",
    "for bucket in buckets:\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b4cdd7",
   "metadata": {},
   "source": [
    "## 3.0 Uploading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc1029",
   "metadata": {},
   "source": [
    "##### uncomment it if you want to upload the dataset in to gcloud bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63e7591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : Upload the dataset from local directory to gcloud bucket names aishield\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : Upload the dataset from local directory to gcloud bucket \n",
    "'''\n",
    "\n",
    "data_path =\"Training_Dataset/data/\"\n",
    "\n",
    "#TODO copy BUCKET_URI in place of {BUCKET_URI}\n",
    "!gsutil cp -r \"Training_Dataset/data/\" \"{BUCKET_URI}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca87b4c",
   "metadata": {},
   "source": [
    "## 4.0 Initialize Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa190c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : to initialize the vertex ai platform sdk\n",
    "'''\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94f947",
   "metadata": {},
   "source": [
    "## 5.0 Train and Download the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de319519",
   "metadata": {},
   "source": [
    "### `5.0.0 Preparing a traning script`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a890ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : to create a training script\n",
    "'''\n",
    "\n",
    "# %%writefile train.py\n",
    "\n",
    "python_code ='''\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, optimizers , Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Dense, Flatten\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from random import shuffle\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from google.cloud import storage\n",
    "\n",
    "img_row,img_col,channel=(32,32,1)\n",
    "num_classes=2\n",
    "input_shape=(img_row,img_col,channel)\n",
    "\n",
    "label_int_str_map = {0:'no_findings',1:'covid'}\n",
    "\n",
    "def prepare_data(file_path,classes=2):\n",
    "    \"\"\"\n",
    "    Description: load data and perform preprocessing  \n",
    "    Args:\n",
    "        file_path:path where data is present\n",
    "        classes: number of class where to load data - \n",
    "    Returns: numpy format data and its label\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    download_path = \"gsutil cp -r \"+file_path+\" .\"\n",
    "    print(\"Downloading files from gcloud bucket\")\n",
    "    os.system(download_path)\n",
    "    print(\"Downloaded data from gcloud bucket\")\n",
    "    path_file = file_path.split(\"/\")[-1]\n",
    "    for i in range(classes):\n",
    "        path = os.path.join(path_file,str(i))\n",
    "        images = os.listdir(path)\n",
    "        for a in images:\n",
    "            try:\n",
    "                img = cv2.imread(\"./\"+os.path.join(path,str(a)), 0)\n",
    "                img = cv2.resize(img, (input_shape[1], input_shape[0])) \n",
    "                img=np.expand_dims(img,axis=-1)\n",
    "                data.append(img)\n",
    "                labels.append(i)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data),np.array(labels) \n",
    "\n",
    "def modelarch():\n",
    "    a=0.07\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=256,kernel_size=(3,3),padding='same',strides=(1,1), activation='relu',input_shape=(32,32,1), \n",
    "                     kernel_regularizer=l2(a), bias_regularizer=l2(a)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters=128,kernel_size=(3,3),padding='same',strides=(1,1), activation='relu',kernel_regularizer=l2(a), bias_regularizer=l2(a)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters=128,kernel_size=(3,3),padding='same',strides=(1,1), activation='relu',kernel_regularizer=l2(a), bias_regularizer=l2(a)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(filters=128,kernel_size=(3,3),padding='same',strides=(1,1), activation='relu',\n",
    "                     kernel_regularizer=l2(a), bias_regularizer=l2(a)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(10,activation='selu', kernel_regularizer=l2(a), bias_regularizer=l2(a)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(num_classes,activation = 'softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(train_data_folder,BUCKET_NAME, PROJECT_ID):\n",
    "    print(\"Preparing data\")\n",
    "    X,y = prepare_data(train_data_folder,classes=num_classes)\n",
    "    print(\"Splitting the dataset\")\n",
    "    X_train,X_test , y_train , y_test= train_test_split(X,y,stratify = y,test_size = 0.2,shuffle=True,random_state = 42)\n",
    "    print(\"Splitting the dataset\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,stratify = y_train,test_size = 0.1,shuffle=True,random_state = 42)\n",
    "    X_train = X_train.reshape(-1,*input_shape)/255.0\n",
    "    X_val =  X_val.reshape(-1,*input_shape)/255.0\n",
    "    X_test =  X_test.reshape(-1,*input_shape)/255.0\n",
    "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "    y_val=keras.utils.to_categorical(y_val,num_classes)\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "    model = modelarch()\n",
    "    \n",
    "    #Compile model\n",
    "    model.compile(optimizer=optimizers.Adam(), loss = 'categorical_crossentropy', metrics=['accuracy']) \n",
    "    \n",
    "    # Checkpoint\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='outputs/model.h5',monitor='val_loss',verbose=1,save_best_only=True,mode='auto')\n",
    "    # Early stopper\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=10,mode='min')\n",
    "\n",
    "    callbacks = [early_stop, checkpoint]\n",
    "\n",
    "    history = model.fit(X_train, y_train, validation_data = (X_val, y_val),epochs = 100, batch_size=16, verbose = 1 , callbacks = callbacks)\n",
    "    \n",
    "    print(\"Finished Training\")\n",
    "    \n",
    "    print(\"saving the model file\")\n",
    "    \n",
    "    model.save('covid_model.h5')\n",
    "    \n",
    "    upload_blob(BUCKET_NAME,'covid_model.h5', 'models/covidexp/covid_model.h5', PROJECT_ID)\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name, PROJECT_ID):\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, destination_blob_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-data-folder', type=str, dest='train_data_folder',\n",
    "    help='data folder mounting point')\n",
    "    parser.add_argument('--project-id', type=str, dest='PROJECT_ID')\n",
    "    parser.add_argument('--bucket-name', type=str, dest='BUCKET_NAME')\n",
    "    args = parser.parse_args()\n",
    "    train_data_folder = args.train_data_folder\n",
    "    BUCKET_NAME = args.BUCKET_NAME\n",
    "    PROJECT_ID = args.PROJECT_ID\n",
    "    print(\"===== DATA =====\")\n",
    "    print('Train Data folder:', train_data_folder)\n",
    "    train(train_data_folder=train_data_folder, BUCKET_NAME = BUCKET_NAME, PROJECT_ID = PROJECT_ID)'''\n",
    "\n",
    "\n",
    "\n",
    "with open(\"train.py\", \"w\") as file:\n",
    "    # Writing data to a file\n",
    "    file.writelines(python_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05ccdb",
   "metadata": {},
   "source": [
    "### `5.0.1 Preparing a Vertex AI Custom containers to run training`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3ac6f",
   "metadata": {},
   "source": [
    "#### `Preparing a Docker file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89073242",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : create a dockerfile\n",
    "'''\n",
    "\n",
    "# %%writefile Dockerfile\n",
    "\n",
    "docker_code = '''\n",
    "FROM tensorflow/tensorflow:2.9.1-gpu\n",
    "WORKDIR /root\n",
    "\n",
    "RUN apt-get install python3\n",
    "RUN apt-get install python3-pip\n",
    "RUN apt update && apt install -y libsm6 libxext6\n",
    "RUN apt-get install -y libxrender-dev\n",
    "\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y curl gnupg && \\\n",
    "    echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\\n",
    "    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && \\\n",
    "    apt-get update -y && \\\n",
    "    apt-get install google-cloud-sdk -y\n",
    "    \n",
    "RUN pip3 install opencv-python numpy==1.22 google-cloud-storage==1.35.0 scikit-learn==1.0.2 pandas==1.1.5 protobuf==3.20.*\n",
    "\n",
    "COPY train.py ./train.py\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"train.py\"]'''\n",
    "\n",
    "with open('Dockerfile', \"w\") as file:\n",
    "    # Writing data to a file\n",
    "    file.writelines(docker_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947df61",
   "metadata": {},
   "source": [
    "#### `Preparing a cloud_build file`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da3291",
   "metadata": {},
   "source": [
    "#### before writing the cloud_build file , replace the < PROJECT_ID > and < BUCKET_NAME >\n",
    "\n",
    "for eg - args: ['build', '-t', 'gcr.io/xyz-sandbox/xyz', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd32510",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : Create a cloub_build file \n",
    "'''\n",
    "\n",
    "# %%writefile cloudfile2.yaml\n",
    "\n",
    "#TODO copy and paste the PROJECT_ID, BUCKET_NAME in place of <PROJECT_ID> and <BUCKET_NAME>\n",
    "\n",
    "cloud_file_code ='''\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/<PROJECT_ID>/<BUCKET_NAME>', '.']\n",
    "    \n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['push', 'gcr.io/<PROJECT_ID>/<BUCKET_NAME>']\n",
    "images:\n",
    "- gcr.io/<PROJECT_ID>/<BUCKET_NAME>'''\n",
    "\n",
    "\n",
    "with open('cloudfile2.yaml', \"w\") as file:\n",
    "    # Writing data to a file\n",
    "    file.writelines(cloud_file_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76b5d1",
   "metadata": {},
   "source": [
    "#### `Run the cloud_build to create a custom image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4283ae67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : to build the custom container image \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : to build the custom container image \n",
    "'''\n",
    "#TODO copy and paste the REGION in place of REGION\n",
    "\n",
    "\n",
    "!gcloud builds submit --config cloudfile2.yaml --region=REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd264ee",
   "metadata": {},
   "source": [
    "### `5.0.2 Creating a Job id to run the traning script`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "56cfb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : Create a JOB name to run the training script\n",
    "'''\n",
    "JOB_NAME = \"covid_job_ais_final_1\"\n",
    "\n",
    "DATA_DIR = BUCKET_URI+\"/data\"\n",
    "# CMDARGS = ['--train-data-folder', DATA_DIR]\n",
    "CMDARGS = [\n",
    "    \"--train-data-folder=\"+DATA_DIR,\n",
    "    \"--project-id=\"+PROJECT_ID,\n",
    "    \"--bucket-name=\"+BUCKET_NAME\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e6cae069",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : Copy the Custom image name with tag created when we ran the Cloud_build file\n",
    "'''\n",
    "\n",
    "#TODO copy and paste the PROJECT_ID,BUCKET_NAME  in place of <PROJECT_ID> and <BUCKET_NAME>\n",
    "\n",
    "CUSTOM_CONTAINER_IMAGE_URI=\"gcr.io/<PROJECT_ID>/<BUCKET_NAME>:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adc71b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : Specify the worker pool configuration , and provide image_uri path and any arguments need to pass\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : Specify the worker pool configuration , and provide image_uri path and any arguments need to pass\n",
    "'''\n",
    "#this below worker_pool_specs uses tensorflow2.9.1-gpu (machine_type and acccelarator_type)\n",
    "\n",
    "worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-8\",\n",
    "                \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "                \"accelerator_count\": 1,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": CUSTOM_CONTAINER_IMAGE_URI,\n",
    "                \"args\" :CMDARGS\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "print(worker_pool_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f479682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : Creating a custom traning job using vertex ai sdk\n",
    "'''\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=f\"{JOB_NAME}\",\n",
    "    worker_pool_specs=worker_pool_specs,    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3dee64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : this cell will run the job id for the custom training \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : this cell will run the job id for the custom training \n",
    "'''\n",
    "job.run(\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbe2c28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : Can found the configuration of job which has run \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : Can found the configuration of job which has run \n",
    "'''\n",
    "job.gca_resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c67ce",
   "metadata": {},
   "source": [
    "### `5.0.3 Download the model saved inside bucket of aishield and save to locally`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a06489f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : to download the model from gcloud bucket to locally\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : to download the model from gcloud bucket to locally\n",
    "'''\n",
    "\n",
    "#TODO copy and paste the BUCKET_URI  in place of BUCKET_URI\n",
    "\n",
    "!gsutil cp -r BUCKET_URI+\"/models/covidexp/\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c4433c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description : to load the training model\n",
    "'''\n",
    "\n",
    "imported_model = tf.keras.models.load_model('covidexp/covid_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ae485ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 256)       2560      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 256)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 16, 16, 256)      1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 128)       295040    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 4, 4, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 2, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 600,480\n",
      "Trainable params: 599,200\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imported_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f26561",
   "metadata": {},
   "source": [
    "### 6.0 AIShield API Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ff8c4",
   "metadata": {},
   "source": [
    "### `6.0.1 Prepare Data , Model and Label zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bac7aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(directory):\n",
    "    \"\"\"\n",
    "    create directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directorys : list containing the directorys path to create \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    for d in directory:\n",
    "        if os.path.isdir(d):\n",
    "            print(\"directory {} already exist\".format(d))\n",
    "        if os.path.isdir(d)==False:\n",
    "            os.mkdir(path=d)\n",
    "            print(\"directory {} created successfully\".format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6337c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_directory(directorys):\n",
    "    \"\"\"\n",
    "    delete directory \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directorys : list containing the directorys to deleate along with all the files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(directorys)>=1:\n",
    "        for d in directorys:\n",
    "            if os.path.isdir(d):\n",
    "                try:\n",
    "                    if os.path.isfile(d):\n",
    "                        os.remove(path=d)\n",
    "                    else:\n",
    "                        shutil.rmtree(path=d)\n",
    "                        print(\"Removed: {}\".format(d))\n",
    "                except:\n",
    "                    print(\"Failed to removed: {}\".format(d))\n",
    "            else:\n",
    "                print(\"Failed to removed: {}\".format(d))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c41f0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_archive(base_name,root_dir,zip_format='zip'):\n",
    "    \"\"\"\n",
    "    created zip for given folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_name : name of zip file\n",
    "    root_dir : directory to archive/zip\n",
    "    zip_format : zip or tar \n",
    "        DESCRIPTION. The default is 'zip'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    shutil.make_archive(base_name=base_name, format=zip_format, root_dir=root_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "466d8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path,classes=2):\n",
    "    \"\"\"\n",
    "    Description: load data and perform preprocessing  \n",
    "    Args:\n",
    "        file_path:path where data is present\n",
    "        classes: number of class where to load data - \n",
    "    Returns: numpy format data and its label\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(classes):\n",
    "        path = os.path.join(file_path,str(i))\n",
    "        images = os.listdir(path)\n",
    "        for a in images:\n",
    "            try:\n",
    "                img = cv2.imread(os.path.join(path,str(a)), 0)\n",
    "                img = cv2.resize(img, (input_shape[1], input_shape[0]))\n",
    "                img=np.expand_dims(img,axis=-1)\n",
    "                data.append(img)\n",
    "                labels.append(i)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "935a12ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription : Create data, model and label folder\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Description : Create data, model and label folder\n",
    "'''\n",
    "data_path=os.path.join(os.getcwd(),\"data\")\n",
    "model_path=os.path.join(os.getcwd(),\"model\")\n",
    "label_path=os.path.join(os.getcwd(),\"label\")\n",
    "zip_path=os.path.join(os.getcwd(),\"zip\")\n",
    "pyc_model_path=os.path.join(os.getcwd(),\"pyc_model\")\n",
    "report_path = os.path.join(os.getcwd(), \"reports\")\n",
    "#deleting folder\n",
    "delete_directory(directorys=[data_path,model_path,label_path,zip_path,pyc_model_path,report_path])\n",
    "\n",
    "#creating folder\n",
    "make_directory([data_path,model_path,label_path,zip_path,pyc_model_path,report_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "73931a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32,32,1)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "05fd5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_path = 'Training_Dataset/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5e9a3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "data , label = prepare_data(training_images_path,classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7c1e732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250, 32, 32, 1), (250,))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape , label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8ac59853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Save data and label\n",
    "\"\"\"\n",
    "labels=pd.DataFrame()\n",
    "img_name = []\n",
    "img_label = [] \n",
    "  \n",
    "for i in range(data.shape[0]):\n",
    "    cv2.imwrite(os.path.join(data_path,str(i)+\".jpg\"),data[i]) # don't use plt.imread otheriwse while loading the saved images , and passing to model there is accuarcy drop \n",
    "    img_name.append(str(i)+\".jpg\")\n",
    "    img_label.append(label[i])\n",
    "labels['image'] = img_name\n",
    "labels[\"label\"] = np.array(img_label)\n",
    "\n",
    "#write orig_label dataframe\n",
    "labels.to_csv(os.path.join(label_path,\"label.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "17962d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Zip data\n",
    "\"\"\"\n",
    "make_archive(base_name=os.path.join(zip_path,\"data\"),root_dir=data_path,zip_format='zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1d31883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Zip label\n",
    "\"\"\"\n",
    "make_archive(base_name=os.path.join(zip_path,\"label\"),root_dir=label_path,zip_format='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e6103fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Zip model\n",
    "\"\"\"\n",
    "shutil.copy('covidexp/covid_model.h5',model_path)\n",
    "make_archive(base_name=os.path.join(zip_path,\"model\"),root_dir=model_path,zip_format='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d9f4a",
   "metadata": {},
   "source": [
    "### `6.0.2 AIShield API Call`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "69c6be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: AIShield API URL and subscription key\n",
    "\"\"\"\n",
    "url = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "headers={'Cache-Control': 'no-cache',\n",
    "'Org-Id': 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',\n",
    "'x-api-key': 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bba9ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id:  XXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: call Model registration api to get unique model it and url to upload data, model and label\n",
    "\"\"\"\n",
    "model_registration_url = url + \"/model_registration/upload\"\n",
    "model_registration_payload = {\n",
    "    'task_type':\"IC\",\n",
    "    \"analysis_type\": \"MEA\"\n",
    "}\n",
    "new_request = requests.request(method=\"POST\", url=model_registration_url, headers=headers, json=model_registration_payload)\n",
    "new_request = json.loads(new_request.text)\n",
    "model_id = new_request['data']['model_id']\n",
    "data_upload_url = new_request['data']['urls']['data_upload_url']\n",
    "label_upload_url = new_request['data']['urls']['label_upload_url']\n",
    "model_upload_url = new_request['data']['urls']['model_upload_url']\n",
    "print('model_id: ', model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "573a8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Files path\n",
    "\"\"\"\n",
    "data_path=os.path.join(zip_path,'data.zip') #full path of data zip\n",
    "label_path=os.path.join(zip_path,'label.zip') #full path of label zip\n",
    "model_path=os.path.join(zip_path,'model.zip') #full path of model zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "32937f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(url, file_path):\n",
    "    new_request = requests.request(method=\"PUT\", url=url, data=open(file_path,'rb'))\n",
    "    status_cd = new_request.status_code\n",
    "    if status_cd == 200:\n",
    "        status = 'upload sucessful'\n",
    "    else:\n",
    "        status = 'upload failed'\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "26a29916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_upload_status:  upload sucessful\n",
      "label_upload_status:  upload sucessful\n",
      "model_upload_status:  upload sucessful\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Hit AIShield File Upload API\n",
    "\"\"\"\n",
    "\n",
    "data_upload_status = upload_file(data_upload_url, data_path)\n",
    "label_upload_status = upload_file(label_upload_url, label_path)\n",
    "model_upload_status = upload_file(model_upload_url, model_path)\n",
    "print('data_upload_status: ', data_upload_status)\n",
    "print('label_upload_status: ', label_upload_status)\n",
    "print('model_upload_status: ', model_upload_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "edd851ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Payload for AIShield VulnerabilityReport api call\n",
    "\"\"\"\n",
    "payload={}\n",
    "payload['model_id']=model_id\n",
    "payload['use_model_api']=\"no\"\n",
    "payload['model_api_details']=\"no\"\n",
    "payload['normalize_data']=\"Yes\"\n",
    "payload['input_dimensions']=str(input_shape)\n",
    "payload['number_of_classes']=str(num_classes)\n",
    "payload['attack_type']=\"blackbox\"  \n",
    "payload['number_of_attack_queries']=10000\n",
    "payload['model_framework']='tensorflow'\n",
    "payload['vulnerability_threshold']=\"0\"\n",
    "payload['defense_bestonly']=\"no\"\n",
    "payload['encryption_strategy']= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf4aa3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* api_version : 1.5\n",
      "* job_id : XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "* monitor_link : XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Hit AIShield VulnerabilityReport api\n",
    "\"\"\"\n",
    "model_analysis_url = url + \"/model_analyse/{}\".format(model_id)\n",
    "if data_upload_status == \"upload sucessful\" and model_upload_status == \"upload sucessful\" and label_upload_status == \"upload sucessful\":\n",
    "    new_request = requests.request(method=\"POST\", url=model_analysis_url, json=payload,headers=headers)\n",
    "    new_request=json.loads(new_request.text)\n",
    "    for k, v in new_request.items():\n",
    "        print(\"* {} : {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a10ba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job id : XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Get job id from api response\n",
    "\"\"\"\n",
    "job_id=new_request['job_id']\n",
    "print(f\"Job id : {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "76b0c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_api_progress(new_job_id):\n",
    "        job_status_url = url + \"/job_status_detailed?job_id=\" + new_job_id\n",
    "\n",
    "        # status dictionary\n",
    "        status_dictionary = {\n",
    "            'ModelExploration_Status': 'na',\n",
    "            'SanityCheck_Status': 'na',\n",
    "            'QueryGenerator_Status': 'na',\n",
    "            'VunerabilityEngine_Status': 'na',\n",
    "            'DefenseReport_Status': 'na',\n",
    "        }\n",
    "        counts = [0] * len(status_dictionary)\n",
    "        failed_api_hit_count = 0\n",
    "        while True:\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                job_status_response = requests.request(\"GET\", job_status_url, params={},\n",
    "                                                       headers=headers)\n",
    "\n",
    "                job_status_payload = json.loads(job_status_response.text)\n",
    "                failing_key = 'ModelExploration_Status'\n",
    "                for i, key in enumerate(status_dictionary.keys()):\n",
    "                    if status_dictionary[key] == 'na':\n",
    "                        if job_status_payload[key] == 'inprogress' and status_dictionary[key] == 'na':\n",
    "                            status_dictionary[key] = job_status_payload[key]\n",
    "                            print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                        elif job_status_payload[key] == 'completed' or job_status_payload[key] == 'passed':\n",
    "                            status_dictionary[key] = job_status_payload[key]\n",
    "                            counts[i] += 1\n",
    "                            print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                        if job_status_payload[key] == 'failed':\n",
    "                            failing_key = key\n",
    "                            status_dictionary[key] = job_status_payload[key]\n",
    "                            print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                    elif job_status_payload[key] == 'completed' or job_status_payload[key] == 'passed':\n",
    "                        status_dictionary[key] = job_status_payload[key]\n",
    "                        if counts[i] < 1:\n",
    "                            print(str(key), \":\", status_dictionary[key])\n",
    "                        counts[i] += 1\n",
    "\n",
    "                    else:\n",
    "                        if job_status_payload[key] == 'failed':\n",
    "                            failing_key = key\n",
    "                            status_dictionary[key] = job_status_payload[key]\n",
    "                            print(str(key), \":\", status_dictionary[key])\n",
    "\n",
    "                if job_status_payload[failing_key] == 'failed':\n",
    "                    break\n",
    "\n",
    "                if status_dictionary['VunerabilityEngine_Status'] == 'passed' or status_dictionary[\n",
    "                    'VunerabilityEngine_Status'] == 'completed' and job_status_payload[\n",
    "                    'CurrentStatus'] == \"Defense generation is not triggered\":\n",
    "                    print(\"\\n Vulnerability score {} failed to cross vulnerability threshold of {}\".format(\n",
    "                        job_status_payload['VulnerabiltyScore'], payload['vulnerability_threshold']))\n",
    "                    break\n",
    "                if job_status_payload['DefenseReport_Status'] == 'completed':\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                failed_api_hit_count += 1\n",
    "                print(\"Error {}. trying {} ...\".format(str(e), failed_api_hit_count))\n",
    "                if failed_api_hit_count >= 3:\n",
    "                    break\n",
    "        return status_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "33a42f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelExploration_Status : completed\n",
      "SanityCheck_Status : passed\n",
      "QueryGenerator_Status : inprogress\n",
      "QueryGenerator_Status : completed\n",
      "VunerabilityEngine_Status : inprogress\n",
      "VunerabilityEngine_Status : completed\n",
      "DefenseReport_Status : inprogress\n",
      "DefenseReport_Status : completed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Continuos monitoring of job progress\n",
    "\"\"\"\n",
    "status_dictionary = monitor_api_progress(new_job_id=job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ce266",
   "metadata": {},
   "source": [
    "### 7.0 Upload the Reports and Artifacts to gcloud bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "490c3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_artifact(job_id, report_type='Vulnerability', file_format=0):\n",
    "    \"\"\"\n",
    "    job_id: job_id  received after successful api call\n",
    "    report_type: report to be downloaded\n",
    "    file_format: change file_format to : 0- all report in zip \n",
    "                        1- report in .txt \n",
    "                        2- report in .pdf\n",
    "                        3- report in .json\n",
    "                        4- report in .xml\n",
    "    \"\"\"\n",
    "    report_url = url + \"/\" + \"get_report?job_id=\" + str(\n",
    "        job_id) + \"&report_type=\" + report_type + \"&file_format=\" + str(file_format)\n",
    "\n",
    "    headers1=headers\n",
    "    headers1[\"content-type\"]= \"application/zip\"\n",
    "\n",
    "    response = requests.request(\"GET\", report_url, params={}, headers=headers1)\n",
    "\n",
    "    if file_format == 0 or \"Attack_samples\":\n",
    "        with open(os.path.join(report_path, report_type + \".zip\"), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    elif file_format == 1:\n",
    "        with open(os.path.join(report_path, report_type + \".txt\"), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    elif file_format == 2:\n",
    "        with open(os.path.join(report_path, report_type + \".pdf\"), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    elif file_format == 3:\n",
    "        with open(os.path.join(report_path, report_type + \".json\"), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    elif file_format == 4:\n",
    "        with open(os.path.join(report_path, report_type + \".xml\"), 'wb') as f:\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c879e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: download generated artifact\n",
    "\"\"\"\n",
    "if status_dictionary[\"VunerabilityEngine_Status\"] == 'completed':\n",
    "    download_artifact(job_id=job_id, report_type='Vulnerability', file_format=0) \n",
    "    download_artifact(job_id=job_id, report_type='Attack_samples', file_format=0)\n",
    "\n",
    "if status_dictionary[\"DefenseReport_Status\"] == 'completed':\n",
    "    download_artifact(job_id=job_id, report_type='Defense', file_format=0)\n",
    "    download_artifact(job_id=job_id, report_type='Defense_artifact', file_format=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "abf0dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_extractor(file, extract_path=None, delete_zip=False):\n",
    "    \"\"\"\n",
    "    extract zip file to the given path\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : path of zip file\n",
    "    extract_path : path to extract zip file, default considered parent directory\n",
    "    delete_zip: True, delete zip file after unzipping it\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    if extract_path is None:\n",
    "        extract_path = os.path.dirname(file)\n",
    "    print(\"Extracting : {}\".format(file))\n",
    "    zf = zipfile.ZipFile(file=file, mode='r')\n",
    "    zf.extractall(extract_path)\n",
    "    zf.close()\n",
    "    if delete_zip:\n",
    "        os.remove(file)\n",
    "        print(\"{} removed successfully.\".format(file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b78b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, destination_blob_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f213d07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Attack_samples.zip', 'Defense.zip', 'Defense_artifact.zip', 'Vulnerability.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(report_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "73d3b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File reports/Attack_samples.zip uploaded to report_artifacts/Attack_samples.zip.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: download generated artifact\n",
    "\"\"\"\n",
    "upload_blob(BUCKET_NAME,'reports/Attack_samples.zip', 'report_artifacts/Attack_samples.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bf81a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File reports/Defense_artifact.zip uploaded to report_artifacts/Defense_artifact.zip.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: upload defense_artifcat to gcloud\n",
    "\"\"\"\n",
    "upload_blob(BUCKET_NAME,'reports/Defense_artifact.zip', 'report_artifacts/Defense_artifact.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b28b1e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File reports/Vulnerability.zip uploaded to report_artifacts/Vulnerability.zip.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: upload vulnerabilirt report to gcloud\n",
    "\"\"\"\n",
    "upload_blob(BUCKET_NAME,'reports/Vulnerability.zip', 'report_artifacts/Vulnerability.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3336b9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File reports/Defense.zip uploaded to report_artifacts/Defense.zip.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: upload defense report to gcloud\n",
    "\"\"\"\n",
    "upload_blob(BUCKET_NAME,'reports/Defense.zip', 'report_artifacts/Defense.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b53807",
   "metadata": {},
   "source": [
    "### 8.0 Deploy the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c57d13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: Extracting defense artifact\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Extracting defense artifact\n",
    "\"\"\"\n",
    "zip_extractor(file=os.path.join(report_path, 'Defense_artifact.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "85526550",
   "metadata": {},
   "outputs": [],
   "source": [
    "defense_model = tf.keras.models.load_model(\"reports/defense_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9a6c5f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " original_img (InputLayer)   [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 30, 30, 16)        160       \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 28, 28, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 9, 9, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 9, 9, 64)          2112      \n",
      "                                                                 \n",
      " global_max_pooling2d_8 (Glo  (None, 64)               0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,042\n",
      "Trainable params: 7,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "defense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "34f146a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_defense\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_defense\\assets\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: save defense model in .pb format\n",
    "\"\"\"\n",
    "\n",
    "defense_model.save('tf_defense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0a6b29ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://tf_defense\\keras_metadata.pb [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/ 15.2 KiB]                                                \n",
      "/ [1 files][ 15.2 KiB/ 15.2 KiB]                                                \n",
      "-\n",
      "Copying file://tf_defense\\saved_model.pb [Content-Type=application/octet-stream]...\n",
      "- [1 files][ 15.2 KiB/152.7 KiB]                                                \n",
      "- [2 files][152.7 KiB/152.7 KiB]                                                \n",
      "\\\n",
      "Copying file://tf_defense\\variables\\variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "\\ [2 files][152.7 KiB/243.5 KiB]                                                \n",
      "\\ [3 files][243.5 KiB/243.5 KiB]                                                \n",
      "|\n",
      "Copying file://tf_defense\\variables\\variables.index [Content-Type=application/octet-stream]...\n",
      "| [3 files][243.5 KiB/245.7 KiB]                                                \n",
      "| [4 files][245.7 KiB/245.7 KiB]                                                \n",
      "/\n",
      "\n",
      "Operation completed over 4 objects/245.7 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: upload the pb format defense model to gcloud bucket\n",
    "\"\"\"\n",
    "#TODO copy and paste the BUCKET_NAME  in place of <BUCKET_NAME>\n",
    "\n",
    "!gsutil cp -r \"tf_defense/\" \"gs://<BUCKET_NAME>/tf_defense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c94015cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: Do a model registry to deloy the model\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: Do a model registry to deloy the model\n",
    "\"\"\"\n",
    "#TODO copy and paste the BUCKET_NAME  in place of <BUCKET_NAME>\n",
    "# here we have used the prebuilt container present in vertext ai for deployment\n",
    "\n",
    "my_model = aiplatform.Model.upload(display_name='covid-defense-model',\n",
    "                                  artifact_uri='gs://<BUCKET_NAME>/tf_defense',\n",
    "                                  serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest',\n",
    "                                  project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fdd0617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: create an endoint to deploy the model \\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: create an endoint to deploy the model \n",
    "\"\"\"\n",
    "\n",
    "endpoint = my_model.deploy(\n",
    "     deployed_model_display_name='my-defense-endpoint',\n",
    "     traffic_split={\"0\": 100},\n",
    "     machine_type=\"n1-standard-4\",\n",
    "     accelerator_count=0,\n",
    "     min_replica_count=1,\n",
    "     max_replica_count=1,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23f56aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: endpoint.display_name\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: endpoint.display_name\n",
    "\"\"\"\n",
    "\n",
    "endpoint.display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c9c3528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: endpoint.gca_resource\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: endpoint.gca_resource\n",
    "\"\"\"\n",
    "endpoint.gca_resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "8e4d3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: reload the endpoint once deployed\n",
    "\"\"\"\n",
    "\n",
    "#TODO copy and paste the endpoint_name, we got from the above cells output\n",
    "\n",
    "endpoint_name = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "359688fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = data[1:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d886a10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: access endpoint for doing prediction\n",
    "\"\"\"\n",
    "\n",
    "print(endpoint.predict(instances=x_test).predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "896a46f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription: undeploy the endpoint once done the prediction\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description: undeploy the endpoint once done the prediction\n",
    "\"\"\"\n",
    "\n",
    "#TODO copy and paste the endpoint_id, we got from the above cells output (id)\n",
    "\n",
    "endpoint_id = \"XXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "endpoint.undeploy(endpoint_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
